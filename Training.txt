Training (4)
Includes Hyperparameters (2)
Report at least 3 types of hyperparameters such as learning rate, batch size, hidden size, and dropout.
Includes Computational requirements (1)
Report at least 3 types of requirements such as type of hardware, average runtime for each epoch, total number of trials, GPU hrs used, and # training epochs.
Includes Training Details (1)
Loss functions
Please use LLMs to help write code for the training loop.
What was the initial prompt that you used? What was the initial output of the LLM? Validate the LLM response. How correct, relevant, and helpful was the LLM? How many prompts did you use?

The training uses Adam optimizer with the following parameters: β₁ = 0.9, β₂ = 0.999, and ε = 10^-7. It uses a learning rate of 0.001. They used an L2 weight regularization with λ = 0.001 and a dropout rate of 0.25. A moderate regularization was used since sleep data have a high noise-to-signal ratio. The dropout rate is on the lower side since polysomnography signals contains several important data and we don't want these to be dropped frequently. They used a batch size of 256. They have early stop for training if there is no improvement in validation loss after 20 epochs. 

The training we used have a max number of 100 epochs with early stopping. Since polysomnography signals have very complex patternts, we want multiple iterations to learn the correlations between the signal channels. The stopping creitera is the validation loss. The average runtine for each epoch is around 15 minutes. The model uses TensorFlow and was implemented with GPU acceleration. Especially for the raw data which contained several EDFs for each sample a large storage is needed since a single EDF file are in the rages of 500mb to 1.5gb. Due to this the preprocessing step is very intensive so a sufficient RAM of 16GB is suggested as well. 

The original research paper uses Binary Cross-Entropy as the loss function since detecting sleep apnea uses a binary classification of apnea and hypopnea or presence and absence in a given epoch. Binary Cross-Entroopy is calculated by BCE(y, ŷ) = -[y·log(ŷ) + (1-y)·log(1-ŷ)] where y is the true label (0 or 1) and ŷ is the predicted probability. It especially works well with physiological signals that can have both very weak or very strong patterns because BCE is stable even when predictions approach 0 or 1. It works well with the L2 regularization model that the research uses to avoid overfitting to noise. 

The initial prompt we used to get Binary Cross-Entropy loss function:
"Please provide the implementation of the Binary Cross-Entropy loss function using keras"

The initial output of the LLM:
pythonfrom tensorflow.keras.losses import BinaryCrossentropy
# Simple implementation using Keras built-in loss
bce_loss = BinaryCrossentropy(from_logits=False)
# When compiling your model:
model.compile(optimizer="adam", 
              loss=bce_loss,
              metrics=['accuracy'])

The response is very relevant and helpful and not too far from the code that we finally used. It took us 2 prompts in order to get the final code with minimal adjustments to metrics.